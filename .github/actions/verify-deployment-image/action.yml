name: Verify Deployment Image
description: Verify that a Kubernetes deployment is using the expected image tag

inputs:
  namespace:
    description: Kubernetes namespace
    required: true
  deployment-name:
    description: Name of the deployment to verify
    required: true
  expected-tag:
    description: Expected image tag
    required: true
  registry:
    description: Container registry
    required: true
  image-name:
    description: Image name (without registry prefix)
    required: true
  timeout-seconds:
    description: Maximum time to wait in seconds
    required: false
    default: '300'
  wait-for-ready:
    description: Wait for deployment to be ready
    required: false
    default: 'true'

runs:
  using: composite
  steps:
    - name: Verify deployment with comprehensive fail-fast diagnostics
      shell: bash
      run: |
        NAMESPACE="${{ inputs.namespace }}"
        DEPLOYMENT="${{ inputs.deployment-name }}"
        EXPECTED_TAG="${{ inputs.expected-tag }}"
        REGISTRY="${{ inputs.registry }}"
        IMAGE_NAME="${{ inputs.image-name }}"
        TIMEOUT=${{ inputs.timeout-seconds }}
        INTERVAL=5
        MAX_ATTEMPTS=$((TIMEOUT / INTERVAL))
        CHECK_INTERVAL=6  # Run detailed checks every 30s (6 * 5s)
        
        echo "üîç Verifying ${DEPLOYMENT} deployment with fail-fast diagnostics..."
        echo "  Expected image: ${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}"
        echo "  Namespace: ${NAMESPACE}"
        echo "  Timeout: ${TIMEOUT}s"
        echo ""
        
        # CRITICAL PRE-CHECK: Verify overlay actually contains the expected image tag
        # This catches cases where Argo CD synced successfully but to a STALE overlay
        APP_NAME=$(echo ${NAMESPACE} | sed 's/preview-/preview-pr-/')
        echo "::group::üìã Pre-check: Verify Argo CD has correct image tag in manifest"
        if kubectl get applications.argoproj.io ${APP_NAME} -n argocd &>/dev/null; then
          # Get the kustomization.yaml from Argo CD's cached manifest
          MANIFEST=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.summary.images}' 2>/dev/null || echo "")
          
          if [[ "$MANIFEST" == *"${EXPECTED_TAG}"* ]]; then
            echo "‚úÖ Argo CD manifest contains expected tag: ${EXPECTED_TAG}"
          else
            echo "::error::‚ùå CRITICAL: Argo CD manifest does NOT contain expected tag!"
            echo "  Expected tag: ${EXPECTED_TAG}"
            echo "  Current images in manifest: ${MANIFEST}"
            echo ""
            echo "  This indicates Argo CD synced a STALE overlay version."
            echo "  Possible causes:"
            echo "  1. Argo CD cache is stale (polling didn't detect overlay update)"
            echo "  2. GitHub authentication issue preventing overlay fetch"
            echo "  3. Overlay update wasn't pushed to PR branch successfully"
            echo ""
            echo "  Triggering hard refresh to force re-fetch..."
            kubectl patch application ${APP_NAME} -n argocd -p '{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}' --type=merge 2>/dev/null || true
            
            # Give Argo CD 30s to refresh
            echo "  Waiting 30s for Argo CD to refresh..."
            sleep 30
            
            # Re-check
            MANIFEST=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.summary.images}' 2>/dev/null || echo "")
            if [[ "$MANIFEST" == *"${EXPECTED_TAG}"* ]]; then
              echo "‚úÖ Manifest updated after refresh"
            else
              echo "::error::‚ùå Manifest still stale after refresh - failing fast"
              kubectl describe application ${APP_NAME} -n argocd
              exit 1
            fi
          fi
        fi
        echo "::endgroup::"
        echo ""
        
        # Helper function: Check Argo CD Application Health
        check_argocd_health() {
          local app_name=$1
          local fail_on_error=${2:-false}
          
          if ! kubectl get applications.argoproj.io ${app_name} -n argocd &>/dev/null; then
            echo "  ‚ö†Ô∏è Argo CD application ${app_name} not found"
            return 1
          fi
          
          local health=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.health.status}' 2>/dev/null || echo "Unknown")
          local sync=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown")
          local target_rev=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.spec.source.targetRevision}' 2>/dev/null || echo "")
          local synced_rev=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.sync.revision}' 2>/dev/null || echo "")
          
          echo "  Argo CD Health: ${health} | Sync: ${sync}"
          
          # FAIL FAST: Check for fatal Argo CD issues
          if [[ "${target_rev}" =~ ^[0-9a-f]{40}$ ]]; then
            echo "::error::‚ùå FATAL: Application locked to commit SHA!"
            echo "  Target revision '${target_rev}' is a commit SHA, not a branch."
            echo "  Argo CD won't detect overlay updates pushed to the PR branch."
            echo "  Fix: ApplicationSet must use '{{branch}}' not '{{head_sha}}'"
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi
          
          # Check for Degraded health
          if [ "$health" = "Degraded" ]; then
            echo "::error::‚ùå Application health is Degraded!"
            local conditions=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.conditions[*].message}' 2>/dev/null)
            if [ -n "$conditions" ]; then
              echo "  Conditions: ${conditions}"
            fi
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi
          
          # Check for Missing resources
          if [ "$health" = "Missing" ]; then
            echo "::error::‚ùå Application resources are Missing!"
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi
          
          # Check sync status
          if [ "$sync" = "OutOfSync" ]; then
            echo "  ‚ö†Ô∏è Application is OutOfSync (may be syncing...)"
            local operation=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.operationState.phase}' 2>/dev/null)
            if [ "$operation" = "Failed" ]; then
              echo "::error::‚ùå Last sync operation Failed!"
              local msg=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.operationState.message}' 2>/dev/null)
              [ -n "$msg" ] && echo "  Error: ${msg}"
              [ "$fail_on_error" = "true" ] && exit 1
              return 1
            fi
          fi
          
          return 0
        }
        
        # Helper function: Check Deployment Health
        check_deployment_health() {
          local namespace=$1
          local deployment=$2
          local fail_on_error=${3:-false}
          
          if ! kubectl get deployment ${deployment} -n ${namespace} &>/dev/null; then
            return 1  # Deployment doesn't exist yet, not an error
          fi
          
          # Get deployment status
          local desired=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")
          local ready=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
          local available=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.availableReplicas}' 2>/dev/null || echo "0")
          local updated=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.updatedReplicas}' 2>/dev/null || echo "0")
          
          echo "  Deployment Status: ${ready}/${desired} ready, ${available}/${desired} available, ${updated}/${desired} updated"
          
          # FAIL FAST: Check for ReplicaFailure condition
          local replica_failure=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="ReplicaFailure")].status}' 2>/dev/null)
          if [ "$replica_failure" = "True" ]; then
            echo "::error::‚ùå FATAL: Deployment has ReplicaFailure condition!"
            local reason=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="ReplicaFailure")].reason}' 2>/dev/null)
            local message=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="ReplicaFailure")].message}' 2>/dev/null)
            echo "  Reason: ${reason}"
            echo "  Message: ${message}"
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi
          
          # Check if deployment is progressing but stuck
          local progressing=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Progressing")].status}' 2>/dev/null)
          local prog_reason=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Progressing")].reason}' 2>/dev/null)
          if [ "$progressing" = "False" ] && [ "$prog_reason" = "ProgressDeadlineExceeded" ]; then
            echo "::error::‚ùå FATAL: Deployment progress deadline exceeded!"
            local message=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Progressing")].message}' 2>/dev/null)
            echo "  Message: ${message}"
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi
          
          return 0
        }
        
        # Helper function: Check Pod Issues
        check_pod_issues() {
          local namespace=$1
          local deployment=$2
          local fail_on_error=${3:-false}
          
          # Get pods for this deployment
          local pods=$(kubectl get pods -n ${namespace} -l app=${deployment} -o name 2>/dev/null)
          if [ -z "$pods" ]; then
            return 0  # No pods yet, not necessarily an error
          fi
          
          local fatal_error=false
          
          # Check each pod for fatal issues
          for pod in $pods; do
            local pod_name=$(echo $pod | cut -d'/' -f2)
            local phase=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.phase}' 2>/dev/null)
            
            # FAIL FAST: Check for ImagePullBackOff / ErrImagePull
            local waiting_reason=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}' 2>/dev/null)
            if [ "$waiting_reason" = "ImagePullBackOff" ] || [ "$waiting_reason" = "ErrImagePull" ]; then
              echo "::error::‚ùå FATAL: Pod ${pod_name} cannot pull image!"
              local message=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].state.waiting.message}' 2>/dev/null)
              echo "  Reason: ${waiting_reason}"
              echo "  Message: ${message}"
              fatal_error=true
            fi
            
            # FAIL FAST: Check for CrashLoopBackOff
            if [ "$waiting_reason" = "CrashLoopBackOff" ]; then
              echo "::error::‚ùå FATAL: Pod ${pod_name} in CrashLoopBackOff!"
              local restart_count=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].restartCount}' 2>/dev/null || echo "0")
              echo "  Restart count: ${restart_count}"
              
              # Show last termination reason
              local term_reason=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}' 2>/dev/null)
              local term_message=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].lastState.terminated.message}' 2>/dev/null)
              local exit_code=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].lastState.terminated.exitCode}' 2>/dev/null)
              
              if [ -n "$term_reason" ]; then
                echo "  Last termination: ${term_reason} (exit ${exit_code})"
                [ -n "$term_message" ] && echo "  Message: ${term_message}"
              fi
              
              # Show recent logs
              echo "  Recent logs:"
              kubectl logs ${pod} -n ${namespace} --tail=20 2>&1 | sed 's/^/    /' || echo "    (cannot retrieve logs)"
              fatal_error=true
            fi
            
            # FAIL FAST: Check for CreateContainerConfigError
            if [ "$waiting_reason" = "CreateContainerConfigError" ]; then
              echo "::error::‚ùå FATAL: Pod ${pod_name} has container config error!"
              local message=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].state.waiting.message}' 2>/dev/null)
              echo "  Message: ${message}"
              fatal_error=true
            fi
            
            # Check for OOMKilled
            local term_reason=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}' 2>/dev/null)
            if [ "$term_reason" = "OOMKilled" ]; then
              echo "::error::‚ùå Pod ${pod_name} was OOMKilled (out of memory)!"
              echo "  Container may need higher memory limits"
              fatal_error=true
            fi
            
            # Check for failed init containers
            local init_waiting=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.initContainerStatuses[0].state.waiting.reason}' 2>/dev/null)
            if [ -n "$init_waiting" ]; then
              echo "::warning::Init container in ${pod_name} is waiting: ${init_waiting}"
              local init_msg=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.initContainerStatuses[0].state.waiting.message}' 2>/dev/null)
              [ -n "$init_msg" ] && echo "  Message: ${init_msg}"
            fi
          done
          
          if [ "$fatal_error" = "true" ]; then
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi
          
          return 0
        }
        
        # ========================================================================
        # PRE-FLIGHT: ACR PULL CAPABILITY CHECK
        # ========================================================================
        # Test if Kubernetes can actually pull from ACR before waiting for deployment
        # This catches auth/permission issues immediately instead of after timeout
        
        echo "::group::üîê Pre-flight: Verify ACR Pull Capability"
        echo "Testing if Kubernetes pods can pull images from ${REGISTRY}.azurecr.io..."
        echo ""
        
        TEST_IMAGE="${REGISTRY}.azurecr.io/${IMAGE_NAME}:${EXPECTED_TAG}"
        TEST_POD_NAME="acr-pull-test-$(date +%s)"
        TEST_NAMESPACE_ORIGINAL="${NAMESPACE}"
        
        # Try to use preview namespace if it exists, otherwise create temp namespace
        if ! kubectl get namespace "${NAMESPACE}" &>/dev/null; then
          echo "  ‚ö†Ô∏è  Target namespace ${NAMESPACE} doesn't exist yet, using default for test"
          TEST_NAMESPACE_ORIGINAL="default"
        fi
        
        # Create a minimal test pod
        cat <<EOF | kubectl apply -n "${TEST_NAMESPACE_ORIGINAL}" -f - &>/dev/null
apiVersion: v1
kind: Pod
metadata:
  name: ${TEST_POD_NAME}
spec:
  restartPolicy: Never
  serviceAccountName: yt-summarizer-sa
  containers:
    - name: test
      image: ${TEST_IMAGE}
      command: ["sh", "-c", "echo 'ACR pull test successful' && exit 0"]
EOF
        
        echo "  ‚è≥ Testing image pull (max 45s)..."
        PULL_TEST_PASSED=false
        
        for test_attempt in {1..45}; do
          POD_PHASE=$(kubectl get pod ${TEST_POD_NAME} -n "${TEST_NAMESPACE_ORIGINAL}" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
          CONTAINER_STATE=$(kubectl get pod ${TEST_POD_NAME} -n "${TEST_NAMESPACE_ORIGINAL}" -o jsonpath='{.status.containerStatuses[0].state}' 2>/dev/null || echo "")
          
          if [ "$POD_PHASE" = "Running" ] || [ "$POD_PHASE" = "Succeeded" ]; then
            echo "  ‚úÖ ACR Pull Test PASSED - Kubernetes can pull images from ACR"
            PULL_TEST_PASSED=true
            break
          elif [[ "$CONTAINER_STATE" == *"ImagePullBackOff"* ]] || [[ "$CONTAINER_STATE" == *"ErrImagePull"* ]]; then
            echo "  ‚ùå ACR Pull Test FAILED - Kubernetes CANNOT pull images from ACR"
            echo ""
            echo "  Failure Details:"
            kubectl describe pod ${TEST_POD_NAME} -n "${TEST_NAMESPACE_ORIGINAL}" 2>/dev/null | grep -A 15 "Events:" | sed 's/^/  /' || echo "  Could not retrieve events"
            echo ""
            
            ERROR_MSG=$(kubectl get pod ${TEST_POD_NAME} -n "${TEST_NAMESPACE_ORIGINAL}" -o jsonpath='{.status.containerStatuses[0].state.waiting.message}' 2>/dev/null || echo "Unknown error")
            
            echo "::error::‚ùå CRITICAL: ACR Authentication/Permission Issue Detected!"
            echo "::error:: "
            echo "::error::  Image: ${TEST_IMAGE}"
            echo "::error::  Error: ${ERROR_MSG}"
            echo "::error:: "
            echo "::error::  Root Cause Analysis:"
            
            if [[ "$ERROR_MSG" == *"401 Unauthorized"* ]] || [[ "$ERROR_MSG" == *"failed to authorize"* ]]; then
              echo "::error::  ‚Üí AKS kubelet identity does NOT have AcrPull permission on ACR"
              echo "::error:: "
              echo "::error::  Fix Options:"
              echo "::error::  1. Grant AcrPull role to AKS kubelet identity:"
              echo "::error::     az role assignment create --assignee <kubelet-identity> \\"
              echo "::error::       --role AcrPull --scope /subscriptions/.../resourceGroups/.../providers/Microsoft.ContainerRegistry/registries/${REGISTRY}"
              echo "::error:: "
              echo "::error::  2. Enable ACR admin credentials and add imagePullSecrets:"
              echo "::error::     - Enable admin on ACR: az acr update -n ${REGISTRY} --admin-enabled true"
              echo "::error::     - Create K8s secret: kubectl create secret docker-registry acr-pull-secret \\"
              echo "::error::         --docker-server=${REGISTRY}.azurecr.io --docker-username=... --docker-password=..."
              echo "::error::     - Add imagePullSecrets to pod specs in manifests"
            elif [[ "$ERROR_MSG" == *"not found"* ]] || [[ "$ERROR_MSG" == *"manifest unknown"* ]]; then
              echo "::error::  ‚Üí Image tag does not exist in ACR (already caught earlier, but double-check)"
            elif [[ "$ERROR_MSG" == *"network"* ]] || [[ "$ERROR_MSG" == *"timeout"* ]]; then
              echo "::error::  ‚Üí Network connectivity issue between AKS and ACR"
              echo "::error::  ‚Üí Check ACR firewall rules and AKS subnet access"
            else
              echo "::error::  ‚Üí Unknown ACR pull issue: ${ERROR_MSG}"
            fi
            
            # Cleanup and fail fast
            kubectl delete pod ${TEST_POD_NAME} -n "${TEST_NAMESPACE_ORIGINAL}" --ignore-not-found=true &>/dev/null || true
            echo "::endgroup::"
            exit 1
          fi
          
          sleep 1
        done
        
        # Cleanup test pod
        kubectl delete pod ${TEST_POD_NAME} -n "${TEST_NAMESPACE_ORIGINAL}" --ignore-not-found=true &>/dev/null || true
        
        if [ "$PULL_TEST_PASSED" = "false" ]; then
          echo "  ‚ö†Ô∏è  ACR pull test timed out (45s) - proceeding anyway but this may indicate slow image pulls"
        fi
        
        echo "::endgroup::"
        echo ""
        
        # ========================================================================
        # MAIN VERIFICATION LOOP WITH FAIL-FAST CHECKS
        # ========================================================================
        
        echo "Starting verification with fail-fast enabled..."
        echo ""
        
        # Get Argo CD app name once
        APP_NAME=$(kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o jsonpath='{.metadata.labels.argocd\.argoproj\.io/instance}' 2>/dev/null || echo "")
        if [ -z "$APP_NAME" ]; then
          # Try to infer from namespace
          APP_NAME=$(echo ${NAMESPACE} | sed 's/^/preview-/')
        fi
        
        for i in $(seq 1 $MAX_ATTEMPTS); do
          # ==================================================================
          # PHASE 1: ARGO CD HEALTH CHECKS (Fail Fast)
          # ==================================================================
          if [ $((i % CHECK_INTERVAL)) -eq 1 ] || [ $i -eq 1 ]; then
            echo "::group::üîç Phase 1: Argo CD Health Check (Attempt $i/${MAX_ATTEMPTS})"
            if ! check_argocd_health "${APP_NAME}" "true"; then
              echo "::endgroup::"
              exit 1
            fi
            echo "::endgroup::"
          fi
          
          # ==================================================================
          # PHASE 2: DEPLOYMENT EXISTENCE & HEALTH (Fail Fast)
          # ==================================================================
          if ! kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} &>/dev/null; then
            if [ $i -eq 1 ]; then
              echo "‚è≥ Waiting for deployment ${DEPLOYMENT} to be created..."
            elif [ $((i % CHECK_INTERVAL)) -eq 0 ]; then
              echo "  Still waiting... (${i}/${MAX_ATTEMPTS})"
              
              # After 30s, check if there's an issue with Argo CD creating resources
              if [ $i -ge 6 ]; then
                echo "::group::üîç Investigating Deployment Creation Delay"
                echo "  Checking Argo CD resource status..."
                local resources=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.resources[*].kind}' 2>/dev/null)
                echo "  Managed resources: ${resources}"
                
                # Check if deployment is in Argo CD's resource list
                if ! echo "${resources}" | grep -q "Deployment"; then
                  echo "::error::‚ùå Argo CD is not managing any Deployments!"
                  echo "  This indicates the kustomization may be missing deployment resources."
                  exit 1
                fi
                echo "::endgroup::"
              fi
            fi
            sleep $INTERVAL
            continue
          fi
          
          if [ $i -eq 1 ] || [ $((i % CHECK_INTERVAL)) -eq 0 ]; then
            echo "::group::üîç Phase 2: Deployment Health Check"
            if ! check_deployment_health "${NAMESPACE}" "${DEPLOYMENT}" "true"; then
              echo "::endgroup::"
              exit 1
            fi
            echo "::endgroup::"
          fi
          
          # ==================================================================
          # PHASE 3: POD HEALTH CHECKS (Fail Fast on Fatal Issues)
          # ==================================================================
          if [ $((i % CHECK_INTERVAL)) -eq 0 ]; then
            echo "::group::üîç Phase 3: Pod Health Check"
            if ! check_pod_issues "${NAMESPACE}" "${DEPLOYMENT}" "true"; then
              echo "::endgroup::"
              exit 1
            fi
            echo "::endgroup::"
          fi
          
          # ==================================================================
          # PHASE 4: IMAGE TAG VERIFICATION
          # ==================================================================
          CURRENT_IMAGE=$(kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null)
          
          if [[ "${CURRENT_IMAGE}" == "${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}" ]]; then
            echo "‚úÖ Deployment ${DEPLOYMENT} has correct image: ${CURRENT_IMAGE}"
            
            # Verify pods are also using correct image
            echo ""
            echo "::group::üîç Pod Image Verification"
            POD_IMAGES=$(kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o jsonpath='{.items[*].spec.containers[0].image}' 2>/dev/null)
            if [ -n "$POD_IMAGES" ]; then
              echo "  Pod images: ${POD_IMAGES}"
              if echo "${POD_IMAGES}" | grep -q "${EXPECTED_TAG}"; then
                echo "  ‚úÖ Pods are using expected tag"
              else
                echo "  ‚ö†Ô∏è Some pods may still be using old image (rolling update in progress)"
              fi
            fi
            echo "::endgroup::"
            
            # ==================================================================
            # PHASE 5: ROLLOUT VERIFICATION (if requested)
            # ==================================================================
            if [ "${{ inputs.wait-for-ready }}" = "true" ]; then
              echo ""
              echo "‚è≥ Waiting for deployment rollout to complete..."
              
              if timeout 120s kubectl rollout status deployment/${DEPLOYMENT} -n ${NAMESPACE}; then
                echo "‚úÖ ${DEPLOYMENT} deployment is ready"
                
                # Final verification
                echo ""
                echo "::group::üìã Final Status Summary"
                echo "--- Deployment ---"
                kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o wide
                
                echo ""
                echo "--- Pods ---"
                kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o wide
                
                # Check for any unhealthy pods
                UNHEALTHY=$(kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o jsonpath='{.items[?(@.status.phase!="Running")].metadata.name}' 2>/dev/null)
                if [ -n "$UNHEALTHY" ]; then
                  echo ""
                  echo "::warning::Unhealthy pods detected: ${UNHEALTHY}"
                  for pod in $UNHEALTHY; do
                    echo "--- Pod ${pod} Status ---"
                    kubectl describe pod ${pod} -n ${NAMESPACE} | tail -30
                  done
                fi
                
                echo "::endgroup::"
                echo ""
                echo "üéâ Verification complete! Deployment is healthy and using correct image."
                exit 0
              else
                # Rollout timeout - comprehensive diagnostics
                echo "::error::‚ùå Deployment rollout timed out after 120s"
                echo ""
                echo "::group::üî¥ Rollout Failure Diagnostics"
                
                echo "--- Deployment Status ---"
                kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o wide
                kubectl describe deployment ${DEPLOYMENT} -n ${NAMESPACE}
                
                echo ""
                echo "--- ReplicaSet Status ---"
                kubectl get replicasets -n ${NAMESPACE} -l app=${DEPLOYMENT} -o wide
                
                echo ""
                echo "--- Pod Status ---"
                kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o wide
                
                echo ""
                echo "--- Pod Details & Events ---"
                for pod in $(kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o name 2>/dev/null); do
                  echo "=== ${pod} ==="
                  kubectl describe ${pod} -n ${NAMESPACE} 2>&1 | tail -50
                done
                
                echo ""
                echo "--- Recent Container Logs ---"
                for pod in $(kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o name 2>/dev/null); do
                  echo "=== Logs from ${pod} ==="
                  kubectl logs ${pod} -n ${NAMESPACE} --tail=100 2>&1 || echo "(cannot retrieve logs)"
                done
                
                echo "::endgroup::"
                exit 1
              fi
            else
              echo ""
              echo "‚úÖ Image verification complete (not waiting for rollout)"
              exit 0
            fi
            
          else
            # Image tag mismatch
            if [ $i -eq 1 ]; then
              echo "‚ö†Ô∏è Image tag mismatch detected"
              echo "  Expected: ${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}"
              echo "  Current:  ${CURRENT_IMAGE}"
              echo "  Waiting for Argo CD to sync..."
            fi
            
            # Periodic detailed diagnostics for mismatch
            if [ $((i % CHECK_INTERVAL)) -eq 0 ]; then
              echo ""
              echo "::group::üîç Image Tag Mismatch Investigation (${i}/${MAX_ATTEMPTS})"
              echo "  Expected: ${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}"
              echo "  Current:  ${CURRENT_IMAGE}"
              echo ""
              
              # Check Argo CD sync status
              SYNC_STATUS=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown")
              TARGET_REV=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.spec.source.targetRevision}' 2>/dev/null || echo "")
              SYNCED_REV=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.sync.revision}' 2>/dev/null || echo "")
              
              echo "  Argo CD Sync: ${SYNC_STATUS}"
              echo "  Target: ${TARGET_REV}"
              echo "  Synced: ${SYNCED_REV:0:8}..."
              
              # Check operation state
              OP_STATE=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.operationState.phase}' 2>/dev/null)
              if [ -n "$OP_STATE" ]; then
                echo "  Operation: ${OP_STATE}"
                
                if [ "$OP_STATE" = "Failed" ]; then
                  echo "::error::‚ùå Argo CD sync operation failed!"
                  OP_MSG=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.operationState.message}' 2>/dev/null)
                  [ -n "$OP_MSG" ] && echo "  Error: ${OP_MSG}"
                  echo "::endgroup::"
                  exit 1
                fi
              fi
              
              # If synced but still wrong image, might be a kustomization issue
              if [ "$SYNC_STATUS" = "Synced" ]; then
                echo ""
                echo "::warning::Argo CD shows 'Synced' but deployment has wrong image!"
                echo "  Possible causes:"
                echo "  - Kustomization.yaml may not have been updated with expected tag"
                echo "  - Image transformation may not be configured correctly"
                echo "  - Argo CD may have synced before overlay was updated"
              fi
              
              echo "::endgroup::"
            fi
          fi
          
          # Progress indicator
          if [ $i -lt $MAX_ATTEMPTS ]; then
            sleep $INTERVAL
          fi
        done
        
        # ========================================================================
        # TIMEOUT - Final diagnostics before failure
        # ========================================================================
        echo ""
        echo "::error::‚ùå Verification timed out after ${TIMEOUT}s"
        echo ""
        echo "::group::üî¥ Timeout Diagnostics"
        
        echo "=== Final Image Status ==="
        echo "Expected: ${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}"
        FINAL_IMAGE=$(kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null || echo "N/A")
        echo "Current:  ${FINAL_IMAGE}"
        
        echo ""
        echo "=== Argo CD Final Status ==="
        kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o yaml 2>/dev/null || echo "Cannot get Argo CD application"
        
        echo ""
        echo "=== Deployment Final Status ==="
        kubectl describe deployment ${DEPLOYMENT} -n ${NAMESPACE} 2>/dev/null || echo "Cannot describe deployment"
        
        echo ""
        echo "=== Pod Final Status ==="
        kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o wide 2>/dev/null || echo "No pods found"
        
        echo "::endgroup::"
        exit 1

