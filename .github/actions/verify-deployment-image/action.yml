name: Verify Deployment Image
description: Verify that a Kubernetes deployment is using the expected image tag

inputs:
  namespace:
    description: Kubernetes namespace
    required: true
  deployment-name:
    description: Name of the deployment to verify
    required: true
  expected-tag:
    description: Expected image tag
    required: true
  registry:
    description: Container registry
    required: true
  image-name:
    description: Image name (without registry prefix)
    required: true
  timeout-seconds:
    description: Maximum time to wait in seconds
    required: false
    default: '300'
  wait-for-ready:
    description: Wait for deployment to be ready
    required: false
    default: 'true'

runs:
  using: composite
  steps:
    - name: Verify deployment with comprehensive fail-fast diagnostics
      shell: bash
      run: |
        NAMESPACE="${{ inputs.namespace }}"
        DEPLOYMENT="${{ inputs.deployment-name }}"
        EXPECTED_TAG="${{ inputs.expected-tag }}"
        REGISTRY="${{ inputs.registry }}"
        IMAGE_NAME="${{ inputs.image-name }}"
        TIMEOUT=${{ inputs.timeout-seconds }}
        INTERVAL=5
        MAX_ATTEMPTS=$((TIMEOUT / INTERVAL))
        CHECK_INTERVAL=6  # Run detailed checks every 30s (6 * 5s)

        echo "ðŸ” Verifying ${DEPLOYMENT} deployment with fail-fast diagnostics..."
        echo "  Expected image: ${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}"
        echo "  Namespace: ${NAMESPACE}"
        echo "  Timeout: ${TIMEOUT}s"
        echo ""

        # CRITICAL PRE-CHECK: Verify overlay actually contains the expected image tag
        # This catches cases where Argo CD synced successfully but to a STALE overlay
        APP_NAME=$(echo ${NAMESPACE} | sed 's/preview-/preview-pr-/')
        echo "::group::ðŸ“‹ Pre-check: Verify Argo CD has correct image tag in manifest"
        if kubectl get applications.argoproj.io ${APP_NAME} -n argocd &>/dev/null; then
          # Get the kustomization.yaml from Argo CD's cached manifest
          MANIFEST=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.summary.images}' 2>/dev/null || echo "")

          if [[ "$MANIFEST" == *"${EXPECTED_TAG}"* ]]; then
            echo "âœ… Argo CD manifest contains expected tag: ${EXPECTED_TAG}"
          else
            echo "::error::âŒ CRITICAL: Argo CD manifest does NOT contain expected tag!"
            echo "  Expected tag: ${EXPECTED_TAG}"
            echo "  Current images in manifest: ${MANIFEST}"
            echo ""
            echo "  This indicates Argo CD synced a STALE overlay version."
            echo "  Possible causes:"
            echo "  1. Argo CD cache is stale (polling didn't detect overlay update)"
            echo "  2. GitHub authentication issue preventing overlay fetch"
            echo "  3. Overlay update wasn't pushed to PR branch successfully"
            echo ""
            echo "  Triggering hard refresh to force re-fetch..."
            kubectl patch application ${APP_NAME} -n argocd -p '{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}' --type=merge 2>/dev/null || true

            # Give Argo CD 30s to refresh
            echo "  Waiting 30s for Argo CD to refresh..."
            sleep 30

            # Re-check
            MANIFEST=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.summary.images}' 2>/dev/null || echo "")
            if [[ "$MANIFEST" == *"${EXPECTED_TAG}"* ]]; then
              echo "âœ… Manifest updated after refresh"
            else
              echo "::error::âŒ Manifest still stale after refresh - failing fast"
              kubectl describe application ${APP_NAME} -n argocd
              exit 1
            fi
          fi
        fi
        echo "::endgroup::"
        echo ""

        # Helper function: Check Argo CD Application Health
        check_argocd_health() {
          local app_name=$1
          local fail_on_error=${2:-false}

          if ! kubectl get applications.argoproj.io ${app_name} -n argocd &>/dev/null; then
            echo "  âš ï¸ Argo CD application ${app_name} not found"
            return 1
          fi

          local health=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.health.status}' 2>/dev/null || echo "Unknown")
          local sync=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown")
          local target_rev=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.spec.source.targetRevision}' 2>/dev/null || echo "")
          local synced_rev=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.sync.revision}' 2>/dev/null || echo "")

          echo "  Argo CD Health: ${health} | Sync: ${sync}"

          # FAIL FAST: Check for fatal Argo CD issues
          if [[ "${target_rev}" =~ ^[0-9a-f]{40}$ ]]; then
            echo "::error::âŒ FATAL: Application locked to commit SHA!"
            echo "  Target revision '${target_rev}' is a commit SHA, not a branch."
            echo "  Argo CD won't detect overlay updates pushed to the PR branch."
            echo "  Fix: ApplicationSet must use '{{branch}}' not '{{head_sha}}'"
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi

          # Check for Degraded health
          if [ "$health" = "Degraded" ]; then
            echo "::error::âŒ Application health is Degraded!"
            local conditions=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.conditions[*].message}' 2>/dev/null)
            if [ -n "$conditions" ]; then
              echo "  Conditions: ${conditions}"
            fi
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi

          # Check for Missing resources
          if [ "$health" = "Missing" ]; then
            echo "::error::âŒ Application resources are Missing!"
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi

          # Check sync status
          if [ "$sync" = "OutOfSync" ]; then
            echo "  âš ï¸ Application is OutOfSync (may be syncing...)"
            local operation=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.operationState.phase}' 2>/dev/null)
            if [ "$operation" = "Failed" ]; then
              echo "::error::âŒ Last sync operation Failed!"
              local msg=$(kubectl get applications.argoproj.io ${app_name} -n argocd -o jsonpath='{.status.operationState.message}' 2>/dev/null)
              [ -n "$msg" ] && echo "  Error: ${msg}"
              [ "$fail_on_error" = "true" ] && exit 1
              return 1
            fi
          fi

          return 0
        }

        # Helper function: Check Deployment Health
        check_deployment_health() {
          local namespace=$1
          local deployment=$2
          local fail_on_error=${3:-false}

          if ! kubectl get deployment ${deployment} -n ${namespace} &>/dev/null; then
            return 1  # Deployment doesn't exist yet, not an error
          fi

          # Get deployment status
          local desired=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")
          local ready=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
          local available=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.availableReplicas}' 2>/dev/null || echo "0")
          local updated=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.updatedReplicas}' 2>/dev/null || echo "0")

          echo "  Deployment Status: ${ready}/${desired} ready, ${available}/${desired} available, ${updated}/${desired} updated"

          # FAIL FAST: Check for ReplicaFailure condition
          local replica_failure=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="ReplicaFailure")].status}' 2>/dev/null)
          if [ "$replica_failure" = "True" ]; then
            echo "::error::âŒ FATAL: Deployment has ReplicaFailure condition!"
            local reason=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="ReplicaFailure")].reason}' 2>/dev/null)
            local message=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="ReplicaFailure")].message}' 2>/dev/null)
            echo "  Reason: ${reason}"
            echo "  Message: ${message}"
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi

          # Check if deployment is progressing but stuck
          local progressing=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Progressing")].status}' 2>/dev/null)
          local prog_reason=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Progressing")].reason}' 2>/dev/null)
          if [ "$progressing" = "False" ] && [ "$prog_reason" = "ProgressDeadlineExceeded" ]; then
            echo "::error::âŒ FATAL: Deployment progress deadline exceeded!"
            local message=$(kubectl get deployment ${deployment} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Progressing")].message}' 2>/dev/null)
            echo "  Message: ${message}"
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi

          return 0
        }

        # Helper function: Check Pod Issues
        check_pod_issues() {
          local namespace=$1
          local deployment=$2
          local fail_on_error=${3:-false}

          # Get pods for this deployment
          local pods=$(kubectl get pods -n ${namespace} -l app=${deployment} -o name 2>/dev/null)
          if [ -z "$pods" ]; then
            return 0  # No pods yet, not necessarily an error
          fi

          local fatal_error=false

          # Check each pod for fatal issues
          for pod in $pods; do
            local pod_name=$(echo $pod | cut -d'/' -f2)
            local phase=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.phase}' 2>/dev/null)

            # FAIL FAST: Check for ImagePullBackOff / ErrImagePull
            local waiting_reason=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}' 2>/dev/null)
            if [ "$waiting_reason" = "ImagePullBackOff" ] || [ "$waiting_reason" = "ErrImagePull" ]; then
              echo "::error::âŒ FATAL: Pod ${pod_name} cannot pull image!"
              local message=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].state.waiting.message}' 2>/dev/null)
              echo "  Reason: ${waiting_reason}"
              echo "  Message: ${message}"
              fatal_error=true
            fi

            # FAIL FAST: Check for CrashLoopBackOff
            if [ "$waiting_reason" = "CrashLoopBackOff" ]; then
              echo "::error::âŒ FATAL: Pod ${pod_name} in CrashLoopBackOff!"
              local restart_count=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].restartCount}' 2>/dev/null || echo "0")
              echo "  Restart count: ${restart_count}"

              # Show last termination reason
              local term_reason=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}' 2>/dev/null)
              local term_message=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].lastState.terminated.message}' 2>/dev/null)
              local exit_code=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].lastState.terminated.exitCode}' 2>/dev/null)

              if [ -n "$term_reason" ]; then
                echo "  Last termination: ${term_reason} (exit ${exit_code})"
                [ -n "$term_message" ] && echo "  Message: ${term_message}"
              fi

              # Show recent logs
              echo "  Recent logs:"
              kubectl logs ${pod} -n ${namespace} --tail=20 2>&1 | sed 's/^/    /' || echo "    (cannot retrieve logs)"
              fatal_error=true
            fi

            # FAIL FAST: Check for CreateContainerConfigError
            if [ "$waiting_reason" = "CreateContainerConfigError" ]; then
              echo "::error::âŒ FATAL: Pod ${pod_name} has container config error!"
              local message=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].state.waiting.message}' 2>/dev/null)
              echo "  Message: ${message}"
              fatal_error=true
            fi

            # Check for OOMKilled
            local term_reason=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}' 2>/dev/null)
            if [ "$term_reason" = "OOMKilled" ]; then
              echo "::error::âŒ Pod ${pod_name} was OOMKilled (out of memory)!"
              echo "  Container may need higher memory limits"
              fatal_error=true
            fi

            # Check for failed init containers
            local init_waiting=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.initContainerStatuses[0].state.waiting.reason}' 2>/dev/null)
            if [ -n "$init_waiting" ]; then
              echo "::warning::Init container in ${pod_name} is waiting: ${init_waiting}"
              local init_msg=$(kubectl get ${pod} -n ${namespace} -o jsonpath='{.status.initContainerStatuses[0].state.waiting.message}' 2>/dev/null)
              [ -n "$init_msg" ] && echo "  Message: ${init_msg}"
            fi
          done

          if [ "$fatal_error" = "true" ]; then
            [ "$fail_on_error" = "true" ] && exit 1
            return 1
          fi

          return 0
        }

        # ========================================================================
        # MAIN VERIFICATION LOOP WITH FAIL-FAST CHECKS
        # ========================================================================

        echo "Starting verification with fail-fast enabled..."
        echo ""

        # Get Argo CD app name once
        APP_NAME=$(kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o jsonpath='{.metadata.labels.argocd\.argoproj\.io/instance}' 2>/dev/null || echo "")
        if [ -z "$APP_NAME" ]; then
          # Try to infer from namespace
          APP_NAME=$(echo ${NAMESPACE} | sed 's/^/preview-/')
        fi

        for i in $(seq 1 $MAX_ATTEMPTS); do
          # ==================================================================
          # PHASE 1: ARGO CD HEALTH CHECKS (Fail Fast)
          # ==================================================================
          if [ $((i % CHECK_INTERVAL)) -eq 1 ] || [ $i -eq 1 ]; then
            echo "::group::ðŸ” Phase 1: Argo CD Health Check (Attempt $i/${MAX_ATTEMPTS})"
            if ! check_argocd_health "${APP_NAME}" "true"; then
              echo "::endgroup::"
              exit 1
            fi
            echo "::endgroup::"
          fi

          # ==================================================================
          # PHASE 2: DEPLOYMENT EXISTENCE & HEALTH (Fail Fast)
          # ==================================================================
          if ! kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} &>/dev/null; then
            if [ $i -eq 1 ]; then
              echo "â³ Waiting for deployment ${DEPLOYMENT} to be created..."
            elif [ $((i % CHECK_INTERVAL)) -eq 0 ]; then
              echo "  Still waiting... (${i}/${MAX_ATTEMPTS})"

              # After 30s, check if there's an issue with Argo CD creating resources
              if [ $i -ge 6 ]; then
                echo "::group::ðŸ” Investigating Deployment Creation Delay"
                echo "  Checking Argo CD resource status..."
                local resources=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.resources[*].kind}' 2>/dev/null)
                echo "  Managed resources: ${resources}"

                # Check if deployment is in Argo CD's resource list
                if ! echo "${resources}" | grep -q "Deployment"; then
                  echo "::error::âŒ Argo CD is not managing any Deployments!"
                  echo "  This indicates the kustomization may be missing deployment resources."
                  exit 1
                fi
                echo "::endgroup::"
              fi
            fi
            sleep $INTERVAL
            continue
          fi

          if [ $i -eq 1 ] || [ $((i % CHECK_INTERVAL)) -eq 0 ]; then
            echo "::group::ðŸ” Phase 2: Deployment Health Check"
            if ! check_deployment_health "${NAMESPACE}" "${DEPLOYMENT}" "true"; then
              echo "::endgroup::"
              exit 1
            fi
            echo "::endgroup::"
          fi

          # ==================================================================
          # PHASE 3: POD HEALTH CHECKS (Fail Fast on Fatal Issues)
          # ==================================================================
          if [ $((i % CHECK_INTERVAL)) -eq 0 ]; then
            echo "::group::ðŸ” Phase 3: Pod Health Check"
            if ! check_pod_issues "${NAMESPACE}" "${DEPLOYMENT}" "true"; then
              echo "::endgroup::"
              exit 1
            fi
            echo "::endgroup::"
          fi

          # ==================================================================
          # PHASE 4: IMAGE TAG VERIFICATION
          # ==================================================================
          CURRENT_IMAGE=$(kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null)

          if [[ "${CURRENT_IMAGE}" == "${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}" ]]; then
            echo "âœ… Deployment ${DEPLOYMENT} has correct image: ${CURRENT_IMAGE}"

            # Verify pods are also using correct image
            echo ""
            echo "::group::ðŸ” Pod Image Verification"
            POD_IMAGES=$(kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o jsonpath='{.items[*].spec.containers[0].image}' 2>/dev/null)
            if [ -n "$POD_IMAGES" ]; then
              echo "  Pod images: ${POD_IMAGES}"
              if echo "${POD_IMAGES}" | grep -q "${EXPECTED_TAG}"; then
                echo "  âœ… Pods are using expected tag"
              else
                echo "  âš ï¸ Some pods may still be using old image (rolling update in progress)"
              fi
            fi
            echo "::endgroup::"

            # ==================================================================
            # PHASE 5: ROLLOUT VERIFICATION (if requested)
            # ==================================================================
            if [ "${{ inputs.wait-for-ready }}" = "true" ]; then
              echo ""
              echo "â³ Waiting for deployment rollout to complete..."

              if timeout 120s kubectl rollout status deployment/${DEPLOYMENT} -n ${NAMESPACE}; then
                echo "âœ… ${DEPLOYMENT} deployment is ready"

                # Final verification
                echo ""
                echo "::group::ðŸ“‹ Final Status Summary"
                echo "--- Deployment ---"
                kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o wide

                echo ""
                echo "--- Pods ---"
                kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o wide

                # Check for any unhealthy pods
                UNHEALTHY=$(kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o jsonpath='{.items[?(@.status.phase!="Running")].metadata.name}' 2>/dev/null)
                if [ -n "$UNHEALTHY" ]; then
                  echo ""
                  echo "::warning::Unhealthy pods detected: ${UNHEALTHY}"
                  for pod in $UNHEALTHY; do
                    echo "--- Pod ${pod} Status ---"
                    kubectl describe pod ${pod} -n ${NAMESPACE} | tail -30
                  done
                fi

                echo "::endgroup::"
                echo ""
                echo "ðŸŽ‰ Verification complete! Deployment is healthy and using correct image."
                exit 0
              else
                # Rollout timeout - comprehensive diagnostics
                echo "::error::âŒ Deployment rollout timed out after 120s"
                echo ""
                echo "::group::ðŸ”´ Rollout Failure Diagnostics"

                echo "--- Deployment Status ---"
                kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o wide
                kubectl describe deployment ${DEPLOYMENT} -n ${NAMESPACE}

                echo ""
                echo "--- ReplicaSet Status ---"
                kubectl get replicasets -n ${NAMESPACE} -l app=${DEPLOYMENT} -o wide

                echo ""
                echo "--- Pod Status ---"
                kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o wide

                echo ""
                echo "--- Pod Details & Events ---"
                for pod in $(kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o name 2>/dev/null); do
                  echo "=== ${pod} ==="
                  kubectl describe ${pod} -n ${NAMESPACE} 2>&1 | tail -50
                done

                echo ""
                echo "--- Recent Container Logs ---"
                for pod in $(kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o name 2>/dev/null); do
                  echo "=== Logs from ${pod} ==="
                  kubectl logs ${pod} -n ${NAMESPACE} --tail=100 2>&1 || echo "(cannot retrieve logs)"
                done

                echo "::endgroup::"
                exit 1
              fi
            else
              echo ""
              echo "âœ… Image verification complete (not waiting for rollout)"
              exit 0
            fi

          else
            # Image tag mismatch
            if [ $i -eq 1 ]; then
              echo "âš ï¸ Image tag mismatch detected"
              echo "  Expected: ${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}"
              echo "  Current:  ${CURRENT_IMAGE}"
              echo "  Waiting for Argo CD to sync..."
            fi

            # Periodic detailed diagnostics for mismatch
            if [ $((i % CHECK_INTERVAL)) -eq 0 ]; then
              echo ""
              echo "::group::ðŸ” Image Tag Mismatch Investigation (${i}/${MAX_ATTEMPTS})"
              echo "  Expected: ${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}"
              echo "  Current:  ${CURRENT_IMAGE}"
              echo ""

              # Check Argo CD sync status
              SYNC_STATUS=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown")
              TARGET_REV=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.spec.source.targetRevision}' 2>/dev/null || echo "")
              SYNCED_REV=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.sync.revision}' 2>/dev/null || echo "")

              echo "  Argo CD Sync: ${SYNC_STATUS}"
              echo "  Target: ${TARGET_REV}"
              echo "  Synced: ${SYNCED_REV:0:8}..."

              # Check operation state
              OP_STATE=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.operationState.phase}' 2>/dev/null)
              if [ -n "$OP_STATE" ]; then
                echo "  Operation: ${OP_STATE}"

                if [ "$OP_STATE" = "Failed" ]; then
                  echo "::error::âŒ Argo CD sync operation failed!"
                  OP_MSG=$(kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o jsonpath='{.status.operationState.message}' 2>/dev/null)
                  [ -n "$OP_MSG" ] && echo "  Error: ${OP_MSG}"
                  echo "::endgroup::"
                  exit 1
                fi
              fi

              # If synced but still wrong image, might be a kustomization issue
              if [ "$SYNC_STATUS" = "Synced" ]; then
                echo ""
                echo "::warning::Argo CD shows 'Synced' but deployment has wrong image!"
                echo "  Possible causes:"
                echo "  - Kustomization.yaml may not have been updated with expected tag"
                echo "  - Image transformation may not be configured correctly"
                echo "  - Argo CD may have synced before overlay was updated"
              fi

              echo "::endgroup::"
            fi
          fi

          # Progress indicator
          if [ $i -lt $MAX_ATTEMPTS ]; then
            sleep $INTERVAL
          fi
        done

        # ========================================================================
        # TIMEOUT - Final diagnostics before failure
        # ========================================================================
        echo ""
        echo "::error::âŒ Verification timed out after ${TIMEOUT}s"
        echo ""
        echo "::group::ðŸ”´ Timeout Diagnostics"

        echo "=== Final Image Status ==="
        echo "Expected: ${REGISTRY}/${IMAGE_NAME}:${EXPECTED_TAG}"
        FINAL_IMAGE=$(kubectl get deployment ${DEPLOYMENT} -n ${NAMESPACE} -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null || echo "N/A")
        echo "Current:  ${FINAL_IMAGE}"

        echo ""
        echo "=== Argo CD Final Status ==="
        kubectl get applications.argoproj.io ${APP_NAME} -n argocd -o yaml 2>/dev/null || echo "Cannot get Argo CD application"

        echo ""
        echo "=== Deployment Final Status ==="
        kubectl describe deployment ${DEPLOYMENT} -n ${NAMESPACE} 2>/dev/null || echo "Cannot describe deployment"

        echo ""
        echo "=== Pod Final Status ==="
        kubectl get pods -n ${NAMESPACE} -l app=${DEPLOYMENT} -o wide 2>/dev/null || echo "No pods found"

        echo "::endgroup::"
        exit 1
